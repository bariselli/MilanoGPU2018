{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "import IPythonMagic\n",
    "from Timer import Timer\n",
    "\n",
    "import pycuda.driver as cuda_driver\n",
    "import pycuda.compiler as cuda_compiler\n",
    "from pycuda.gpuarray import GPUArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global logger already initialized!\n"
     ]
    }
   ],
   "source": [
    "%setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registering context in user workspace\n",
      "Creating context\n",
      "PyCUDA version 2018.1.1\n",
      "CUDA version (9, 1, 0)\n",
      "Driver version 10000\n",
      "Using 'Tesla K80' GPU\n",
      " => compute capability: (3, 7)\n",
      " => memory: 10699 / 11441 MB available\n",
      "Created context handle <52426688>\n",
      "Using CUDA cache dir /home/ubuntu/jupyter_notebooks/Stefano_B/MilanoGPU2018/notebooks/cuda_cache\n"
     ]
    }
   ],
   "source": [
    "%cuda_context_handler context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_src = \"\"\"\n",
    "\n",
    "__global__ void shmemReduction(float* output, float* input, int size) {\n",
    "    // 1) stride through global memory and compute the maximum for every thread\n",
    "    int gid = blockIdx.x*blockDim.x + threadIdx.x; // note: blockIdx.x is always zero, because we use only one block\n",
    "    \n",
    "    // float max = 0; // IS IT THE MINIMUM? BETTER THIS:\n",
    "    // float max_value = -FLT_MAX;\n",
    "    float max_value = -9999999;\n",
    "    for (int i = threadIdx.x; i < size; i = i + blockDim.x) {\n",
    "        max_value = fmaxf(max_value, input[i]);\n",
    "    }\n",
    "    \n",
    "    // tmp write to memory to check if things work so far\n",
    "    output[threadIdx.x] = max_value;\n",
    "    \n",
    "    // 2) store the per-thread maximum in shared memory\n",
    "    __shared__ float max_shared[32];\n",
    "    max_shared[threadIdx.x] = max_value;\n",
    "    \n",
    "    // 3) synchronize so that all threads see the same shared memory\n",
    "    __syncthreads();\n",
    "    \n",
    "    // 4) find maximum in shared memory\n",
    "    \n",
    "    // Reduce from 32 to 16 elements\n",
    "    if(threadIdx.x < 16) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x+16]);\n",
    "    }\n",
    "    \n",
    "    // Reduce from 16 to 8 elements\n",
    "    if(threadIdx.x < 8) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x+8]);\n",
    "    }\n",
    "        \n",
    "    // Reduce from 8 to 4 elements\n",
    "    if(threadIdx.x < 4) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x+4]);\n",
    "    }    \n",
    "    \n",
    "    // Reduce from 4 to 2 elements\n",
    "    if(threadIdx.x < 2) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x+2]);\n",
    "    }    \n",
    "    \n",
    "    // Reduce from 2 to 1 elements\n",
    "    if(threadIdx.x < 1) {\n",
    "        max_shared[threadIdx.x] = fmaxf(max_shared[threadIdx.x], max_shared[threadIdx.x+1]);\n",
    "    }    \n",
    "    \n",
    "    // 5) write to output\n",
    "    if(threadIdx.x == 0) {\n",
    "      output[0] = max_shared[0];\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "kernel_module = cuda_compiler.SourceModule(kernel_src)\n",
    "kernel_function = kernel_module.get_function(\"shmemReduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 64\n",
    "a = np.random.random((1, n)).astype(np.float32)\n",
    "\n",
    "a_g = GPUArray(a.shape, a.dtype)\n",
    "a_g.set(a)\n",
    "\n",
    "num_threads = 32\n",
    "b = np.empty((1,num_threads), dtype=np.float32)\n",
    "\n",
    "b_g = GPUArray(b.shape, b.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83689755 0.41948807 0.87312925 0.29224232 0.9481355  0.4531257\n",
      "  0.62819946 0.61715794 0.7371609  0.7659136  0.28334272 0.39048427\n",
      "  0.8993981  0.44780144 0.79649544 0.5367203  0.72738224 0.25699103\n",
      "  0.14175162 0.54631895 0.24623673 0.728681   0.5996733  0.23144549\n",
      "  0.20511219 0.5677549  0.9835198  0.10674427 0.5630324  0.2249372\n",
      "  0.01246603 0.7465469  0.71590334 0.6645421  0.22940327 0.08933225\n",
      "  0.7106722  0.1794032  0.26399305 0.66364807 0.51912904 0.43102342\n",
      "  0.86559397 0.29227048 0.23926395 0.10237733 0.381412   0.11328582\n",
      "  0.6486532  0.6512805  0.70482665 0.98614544 0.5299481  0.03833594\n",
      "  0.12147699 0.25330898 0.4995537  0.44192547 0.32496405 0.75397974\n",
      "  0.65167975 0.08425208 0.09387032 0.89053136]]\n",
      "[[0.98614544 0.6645421  0.87312925 0.29224232 0.9481355  0.4531257\n",
      "  0.62819946 0.66364807 0.7371609  0.7659136  0.86559397 0.39048427\n",
      "  0.8993981  0.44780144 0.79649544 0.5367203  0.72738224 0.6512805\n",
      "  0.70482665 0.98614544 0.5299481  0.728681   0.5996733  0.25330898\n",
      "  0.4995537  0.5677549  0.9835198  0.75397974 0.65167975 0.2249372\n",
      "  0.09387032 0.89053136]]\n",
      "0.98614544\n"
     ]
    }
   ],
   "source": [
    "block_size = (num_threads, 1, 1)\n",
    "grid_size  = (1, 1, 1)\n",
    "\n",
    "kernel_function(b_g, a_g, np.int32(n), grid=grid_size, block=block_size)\n",
    "b_g.get(b)\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "print(np.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
